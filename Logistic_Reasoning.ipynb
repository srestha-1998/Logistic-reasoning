{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Reasoning (Theory)\n",
        "\n",
        "Q1.What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "ANS. Logistic Reasoning (Theory)\n",
        "\n",
        "Q1.What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "ANS. Logistic Regression is a statistical model used for binary classification problems. It predicts the probability that a given input belongs to a particular class (e.g., 0 or 1, Yes or No).\n",
        "\n",
        "Key differences from Linear Regression:\n",
        "\n",
        "Output: Linear Regression predicts a continuous output value, while Logistic Regression predicts a probability (a value between 0 and 1) which is then mapped to a discrete class label.\n",
        "\n",
        "Nature of Problem: Linear Regression is used for regression tasks (predicting continuous values), while Logistic Regression is used for classification tasks (predicting categorical labels).\n",
        "\n",
        "Underlying Function: Linear Regression uses a linear function to model the relationship between inputs and output. Logistic Regression uses a sigmoid (or logistic) function to map the output of a linear combination of inputs to a probability.\n",
        "\n",
        "Cost Function: Linear Regression typically uses Mean Squared Error (MSE) as the cost function. Logistic Regression uses cross-entropy or log loss.\n",
        "\n",
        "Assumptions: Linear Regression assumes a linear relationship between independent and dependent variables, normality of residuals, and homoscedasticity.\n",
        "Logistic Regression does not require a linear relationship between the independent and dependent variables, nor does it assume normality of residuals or homoscedasticity.\n",
        "However, it does assume that the independent variables are not highly correlated (multicollinearity).\n",
        "\n",
        "Q2.What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "ANS. The mathematical equation of logistic regression is based on the logistic function, also known as the sigmoid function. This function transforms the output of a linear regression model into a probability value between 0 and 1. The logistic regression model can be expressed as:\n",
        "\n",
        "p=1 /\n",
        "1+e\n",
        "(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "k\n",
        "​\n",
        " X\n",
        "k\n",
        "​\n",
        " )\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "Here, $ p $ represents the probability of the event occurring, $ \\beta_0 $ is the intercept, and $ \\beta_1, \\beta_2, \\ldots, \\beta_k $ are the coefficients for the predictor variables $ X_1, X_2, \\ldots, X_k $. This equation models the log-odds of the event as a linear combination of the predictor variables.\n",
        "\n",
        "Another way to express the logistic regression model is through the log-odds (logit) form:\n",
        "\n",
        "log( p/1−p\n",
        "\n",
        "\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "k\n",
        "​\n",
        " X\n",
        "k\n",
        "​\n",
        "\n",
        "\n",
        "This equation states that the natural logarithm of the odds of the event is a linear function of the predictor variables.\n",
        " The logistic function ensures that the predicted probabilities are bounded between 0 and 1, making it suitable for binary classification tasks.\n",
        "\n",
        " Q3.Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        " ANS. The Sigmoid function is used in Logistic Regression because it maps any real-valued number to a value between 0 and 1, which is ideal for modeling probabilities.\n",
        " This function transforms the output of a linear regression into a probability, making it suitable for binary classification tasks.\n",
        " The Sigmoid function's S-shaped curve allows it to effectively model the probability of a binary outcome, such as 0 or 1.\n",
        " Additionally, the Sigmoid function is differentiable and continuous, which is beneficial for the optimization processes used in training the model.\n",
        " The function's ability to convert the raw output of the model into a probability value between 0 and 1 is crucial for making predictions in logistic regression.\n",
        "\n",
        " Q4.What is the cost function of Logistic Regression?\n",
        "\n",
        " ANS.Logistic Regression uses a specific cost function to evaluate the performance of the model. Unlike Linear Regression, which uses Mean Squared Error (MSE), Logistic Regression employs a different approach due to the nature of classification problems. The cost function for Logistic Regression is typically referred to as Log Loss or Cross-Entropy Loss.\n",
        " This function is designed to penalize incorrect predictions more severely, especially when the predicted probability is far from the actual label. The cost function (J) for m training samples can be written as:\n",
        "\n",
        "J(θ)=\n",
        "m\n",
        "1\n",
        "​\n",
        " ∑\n",
        "i=1\n",
        "m\n",
        "​\n",
        " [−y\n",
        "(i)\n",
        " log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))−(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "\n",
        "Here, $h_{\\theta}(x)$ represents the hypothesis function, which is the sigmoid function applied to the linear combination of the input features and parameters. The sigmoid function ensures that the output is between 0 and 1, making it suitable for predicting probabilities in binary classification tasks.\n",
        " The use of Log Loss is crucial because it helps in optimizing the parameters of the model to minimize the error in predictions, thereby improving the model's accuracy.\n",
        "\n",
        "Q5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "ANS. Regularization in logistic regression refers to techniques used to prevent overfitting by adding a penalty term to the loss function, which helps to reduce the complexity of the model and improve its generalization to new data.\n",
        " This is achieved by modifying the loss function with a penalty term that shrinks the estimates of the coefficients, thereby introducing bias to the model and decreasing its variance.\n",
        " Regularization is needed because when a model has too many predictor variables relative to the number of samples, it can become overly flexible and fit the training data too closely, leading to poor performance on unseen data.\n",
        " By limiting the flexibility of the model, regularization helps to ensure that the model performs well on both the training and test datasets.\n",
        "\n",
        " Q6.Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        " ANS. Lasso, Ridge, and Elastic Net regression are regularization techniques used in machine learning to prevent overfitting in linear regression models. Each method applies a different type of penalty to the coefficients of the model, which affects how the model handles feature selection and coefficient shrinkage.\n",
        "\n",
        "Lasso regression, also known as L1 regularization, adds a penalty equal to the absolute value of the magnitude of coefficients.\n",
        " This penalty has the effect of reducing some coefficients to zero, effectively performing feature selection by eliminating irrelevant features.\n",
        " Lasso is particularly useful when the goal is to identify the most important features in the dataset.\n",
        " However, Lasso can sometimes remove useful features if not tuned properly.\n",
        "\n",
        "Ridge regression, also known as L2 regularization, adds a penalty equal to the square of the magnitude of coefficients.\n",
        " Unlike Lasso, Ridge does not set coefficients to zero but rather reduces their magnitude, which helps in handling multicollinearity by stabilizing the coefficient estimates.\n",
        " Ridge is effective when all features are relevant but their impact needs to be reduced.\n",
        " However, Ridge keeps all features, which may not be ideal in high-dimensional data with many irrelevant features.\n",
        "\n",
        "Elastic Net regression is a hybrid of Lasso and Ridge regression, combining both L1 and L2 penalties.\n",
        " This method allows for a balance between feature selection and coefficient shrinkage, making it suitable for datasets where features are correlated and feature selection is needed.\n",
        " Elastic Net can handle both multicollinearity and feature selection, as it can select a group of correlated predictors instead of dropping them or choosing one arbitrarily.\n",
        " The penalty term in Elastic Net is a weighted sum of the L1 and L2 norms of the coefficients, where the weight is controlled by a parameter called alpha.\n",
        " When alpha is zero, Elastic Net reduces to Ridge regression, and when alpha is one, it reduces to Lasso regression.\n",
        "\n",
        "In summary, Lasso regression is best for feature selection, Ridge regression is effective for handling multicollinearity, and Elastic Net combines the strengths of both methods to handle correlated features and perform feature selection.\n",
        " Each method has its own strengths and weaknesses, and the choice of method depends on the specific characteristics of the dataset and the goals of the analysis.\n",
        "\n",
        " Q7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        " ANS. Elastic Net is preferred over Lasso or Ridge regression in specific scenarios. It is particularly useful when dealing with highly correlated predictors, as it can outperform Lasso in such cases.\n",
        " Elastic Net combines the strengths of both Lasso and Ridge, allowing it to perform feature selection like Lasso and handle multicollinearity like Ridge.\n",
        " This makes it suitable when there are many correlated features, as it can select a subset of these features and avoid the instability associated with Lasso.\n",
        " Additionally, Elastic Net provides a balance between bias and variance, making it a versatile choice when the data has complex relationships between features.\n",
        " If neither Lasso nor Ridge provides satisfactory results, Elastic Net might offer better predictions.\n",
        "\n",
        " Q8.What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        " ANS. The regularization parameter (λ) in logistic regression controls the strength of the regularization applied to the model. A higher value of λ increases the strength of the regularization, which leads to smaller coefficient values and a simpler model. This helps prevent overfitting by reducing the model's complexity.\n",
        " Conversely, a lower value of λ weakens the regularization, allowing the model to fit the training data more closely, which can lead to overfitting if not controlled properly.\n",
        " The λ parameter is crucial in balancing the trade-off between fitting the training data well and ensuring the model generalizes well to new, unseen data.\n",
        "\n",
        " Q9.What are the key assumptions of Logistic Regression?\n",
        "\n",
        " ANS.Logistic Regression has several key assumptions that must be met for the model to be valid and reliable. First, the response variable should only take on two possible outcomes, such as pass/fail or male/female.\n",
        " Second, the observations in the dataset should be independent of each other.\n",
        " Third, there should be no severe multicollinearity among the explanatory variables.\n",
        " Additionally, the logit of the outcome variable should be a linear combination of the independent variables.\n",
        " Finally, there should be an adequate number of events per independent variable to avoid an overfit model, with commonly recommended minimum \"rules of thumb\" ranging from 10 to 20 events per covariate.\n",
        "\n",
        " Q10.What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        " ANS.Logistic regression is a popular method for classification tasks, but there are several alternatives available depending on the specific requirements and characteristics of the data. Some of these alternatives include:\n",
        "\n",
        "Probit Regression: Similar to logistic regression, but it uses a different link function, specifically the cumulative distribution function of the standard normal distribution.\n",
        "\n",
        "Multinomial Logistic Regression: An extension of binary logistic regression used for multi-class classification problems where the dependent variable has three or more possible categories that are not ordered.\n",
        "\n",
        "Discriminant Analysis: A statistical method used for classification tasks that models the relationship between a categorical dependent variable and one or more continuous independent variables.\n",
        "\n",
        "Decision Trees: A type of supervised learning algorithm that can handle both numerical and categorical data, and can perform both regression and classification. They split the data into smaller subsets based on rules or criteria.\n",
        "\n",
        "Random Forests: An ensemble learning method that constructs multiple decision trees and combines their outputs to improve the accuracy and robustness of the predictions.\n",
        "\n",
        "Support Vector Machines (SVM): A powerful method for classification tasks that finds the optimal hyperplane that maximally separates the classes in the feature space.\n",
        "\n",
        "Neural Networks: A flexible approach that can model complex relationships between inputs and outputs. They are particularly useful for high-dimensional data and can capture non-linear patterns.\n",
        "\n",
        "Tree-based Methods: Such as Classification and Regression Trees (CART), which automatically generate trees from data and are useful for assessing risk factors.\n",
        "\n",
        "Generalized Additive Models (GAMs): These models allow more flexibility than logistic regression by using splines to model the predictors, making them suitable for non-linear relationships.\n",
        "\n",
        "Q11.What are Classification Evaluation Metrics?\n",
        "\n",
        "ANS.Classification evaluation metrics are used to measure the performance of machine learning models in classification tasks. These metrics help assess how well a model predicts the correct class labels for given input data. Some of the commonly used classification evaluation metrics include accuracy, precision, recall, F1 score, and the confusion matrix.\n",
        "\n",
        "Accuracy is a metric that measures how many predictions the model gets correct out of all predictions made. It is calculated as the ratio of correct predictions to the total number of predictions.\n",
        " However, accuracy can be misleading in imbalanced datasets where one class dominates the others.\n",
        "\n",
        "Precision is a metric that measures how many of the predicted positive instances are actually positive. It is calculated as the ratio of true positives to the sum of true positives and false positives.\n",
        " Precision is useful when the cost of false positives is high.\n",
        "\n",
        "Recall, also known as the true positive rate, measures how many of the actual positive instances are correctly identified by the model. It is calculated as the ratio of true positives to the sum of true positives and false negatives.\n",
        " Recall is useful when the cost of false negatives is high.\n",
        "\n",
        "The F1 score is a harmonic mean of precision and recall, providing a single metric that balances both. It is useful when you want to find a balance between precision and recall.\n",
        "\n",
        "The confusion matrix is a table that summarizes the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. The confusion matrix is useful for understanding the types of errors made by the model.\n",
        "\n",
        "Other metrics include the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC). The ROC curve plots the true positive rate against the false positive rate at various threshold settings. The AUC is a single number that summarizes the overall performance of the model.\n",
        "\n",
        "In addition to these metrics, there are other metrics such as the F-beta score, which is a generalization of the F1 score, and the Brier score, which measures the accuracy of probabilistic predictions.\n",
        " The choice of metric depends on the specific problem and the costs associated with different types of errors. For example, in medical diagnosis, minimizing false negatives might be more important than minimizing false positives.\n",
        "\n",
        " Q12.How does class imbalance affect Logistic Regression?\n",
        "\n",
        " ANS.Class imbalance can significantly affect logistic regression models. In two-class classification problems, when the less frequent (minority) class is observed much less than the majority class, logistic regression may not perform optimally.\n",
        " This issue is common in areas like modeling default or fraud detection. Recent work has shown that, in a theoretical context related to infinite imbalance, logistic regression behaves in such a way that all data in the rare class can be replaced by their mean vector to achieve the same coefficient estimates.\n",
        "\n",
        "This characteristic can lead to problems if there is structure within the rare class that is not captured by the mean vector. Logistic regression may not be able to provide the best out-of-sample predictive performance, and an approach that can model underlying structure in the minority class is often superior.\n",
        "\n",
        "Moreover, class imbalance can affect the estimate of the model intercept, which skews all the predicted probabilities. This can compromise predictions, and correcting the intercept is straightforward if the true proportion of 0s and 1s is known.\n",
        "\n",
        "In practice, techniques such as weighted logistic regression can be used to handle imbalanced datasets by assigning different weights to each class based on their prevalence in the dataset.\n",
        " This can improve the performance on the minority class, as shown by higher recall for the minority class compared to standard logistic regression.\n",
        "\n",
        "It is also important to note that while class imbalance itself may not be the primary issue, the lack of sufficient patterns from the minority class can lead to poor representation of its distribution, affecting model performance.\n",
        "\n",
        "Q13.What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "ANS.Hyperparameter tuning is the practice of identifying and selecting the optimal hyperparameters for use in training a machine learning model. When performed correctly, hyperparameter tuning minimizes the loss function of a machine learning model, which means that the model performance is trained to be as accurate as possible. Hyperparameter tuning is an experimental practice, with each iteration testing different hyperparameter values until the best ones are identified. This process is critical to the performance of the model as hyperparameters govern its learning process. The amount of neurons in a neural network, a generative AI model’s learning rate and a support vector machine’s kernel size are all examples of hyperparameters. Good hyperparameter tuning means a stronger performance overall from the machine learning model according to the metrics for its intended task.\n",
        " In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, which must be configured before the process starts. Hyperparameter optimization determines the set of hyperparameters that yields an optimal model which minimizes a predefined loss function on a given data set. The objective function takes a set of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance, and therefore choose the set of values for hyperparameters that maximize it. The traditional method for hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning.\n",
        " Unfortunately, there are no set rules on which hyperparameters work best nor their optimal or default values. You need to experiment to find the optimum hyperparameter set. This activity is known as hyperparameter tuning or hyperparameter optimization. Hyperparameters directly control model structure, function, and performance. Hyperparameter tuning allows data scientists to tweak model performance for optimal results. This process is an essential part of machine learning, and choosing appropriate hyperparameter values is crucial for success. For example, assume you're using the learning rate of the model as a hyperparameter. If the value is too high, the model may converge too quickly with suboptimal results. Whereas if the rate is too low, training takes too long and results may not converge. A good and balanced choice of hyperparameters results in accurate models and excellent model performance.\n",
        " The goal of hyperparameter tuning is to find the values that lead to the best performance on a given task. These settings can affect both the speed and quality of the model's performance. A high learning rate can cause the model to converge too quickly possibly skipping over the optimal solution. A low learning rate might lead to slower convergence and require more time and computational resources. Different models have different hyperparameters and they need to be tuned accordingly. Models can have many hyperparameters and finding the best combination of parameters can be treated as a search problem. The two best strategies for Hyperparameter tuning are: GridSearchCV is a brute-force technique for hyperparameter tuning.\n",
        "\n",
        " Q14.What are different solvers in Logistic Regression? Which one should be used.\n",
        "ANS. The different solvers in logistic regression include liblinear, lbfgs, sag, saga, newton-cg, and newton-cholesky. Each solver has its own characteristics and is suitable for different scenarios.\n",
        "\n",
        "liblinear: This solver is suitable for small datasets and supports both L1 and L2 regularization. It works with binary classification and can use the one-vs-rest strategy for multiclass problems.\n",
        "lbfgs: This is the default solver in scikit-learn and is suitable for medium to large datasets, especially dense ones. It supports L2 regularization and is generally recommended for most cases.\n",
        "sag: This solver is suitable for large datasets with similarly scaled features and supports only L2 regularization. It is known for its fast convergence.\n",
        "saga: This solver is also suitable for large datasets and supports both L1 and L2 regularization, as well as elastic net. It is a good choice when elastic net regularization is required.\n",
        "newton-cg: This solver is suitable for multiclass classification problems and supports multinomial loss functions. It may be slower than lbfgs.\n",
        "newton-cholesky: This is an optimized variant of newton-cg designed for highly structured problems. It can be used for n_samples >> n_features * n_classes, especially with one-hot encoded categorical features with rare categories.\n",
        "For small datasets, liblinear is a good choice, whereas sag and saga are faster for large ones. For multiclass problems, all solvers except liblinear minimize the full multinomial loss. Liblinear can only handle binary classification by default, but it can be extended to handle multiclass problems by using OneVsRestClassifier.\n",
        "\n",
        "The choice of solver depends on the specific requirements of the problem, such as dataset size, regularization type, and computational resources. It is recommended to choose a solver that matches the dataset size and regularization requirements.\n",
        "\n",
        "Q15.How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "ANS.Logistic regression is extended for multiclass classification through several strategies. By design, logistic regression is limited to binary classification, but it can be adapted for multiclass problems using methods such as One-vs-Rest (OvR), One-vs-One (OvO), and Multinomial (softmax) approaches.\n",
        "\n",
        "The One-vs-Rest (OvR) strategy involves training a separate logistic regression model for each class, where each model compares one class against all the others. This is also known as the One-vs-All (OvA) strategy.\n",
        " For example, if there are four classes (e.g., Cat, Dog, Monkey, Bear), this strategy would create four models, each trained to distinguish one class from the rest.\n",
        "\n",
        "Another approach is the One-vs-One (OvO) strategy, which involves training a model for each pair of classes. This results in a larger number of models, specifically C(C-1)/2 models for C classes, and the final prediction is made based on the majority vote of these models.\n",
        "\n",
        "The Multinomial approach, also known as the softmax approach, extends logistic regression natively to handle multiclass classification. Instead of using the sigmoid function, it uses the softmax function to calculate probabilities for multiple outcomes. The softmax function ensures that the probabilities for all classes sum to 1, making it suitable for multiclass problems.\n",
        " This method is supported in libraries like scikit-learn, where the 'multi_class' option can be set to 'multinomial'.\n",
        "\n",
        "In practice, the choice of method depends on the specific requirements of the problem, such as the number of classes, the size of the dataset, and the computational resources available. For instance, the One-vs-Rest approach is simpler and computationally less intensive, while the One-vs-One approach can be more accurate but requires more models.\n",
        " The Multinomial approach is often preferred for its ability to handle multiclass problems directly and efficiently.\n",
        "\n",
        " Q16.What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        " ANS.Logistic regression is a widely used statistical method for classification tasks, particularly for predicting binary outcomes. It has several advantages and disadvantages that are important to consider when deciding whether to use it for a particular problem.\n",
        "\n",
        "Advantages of Logistic Regression\n",
        "Ease of Implementation and Interpretation: Logistic regression is easier to implement, interpret, and train compared to other machine learning methods. It provides a clear understanding of how each predictor variable influences the outcome, making it a popular choice for many applications.\n",
        "Efficiency: It is computationally efficient and fast at classifying unknown records, making it suitable for large datasets.\n",
        "Probabilistic Output: Logistic regression outputs well-calibrated probabilities along with classification results. This allows for nuanced decision-making, as it provides a measure of confidence in the predictions.\n",
        "Linear Boundary Construction: It makes no assumptions about the distributions of classes in feature space, which can be beneficial in certain scenarios.\n",
        "Multinomial Extension: Logistic regression can be extended to handle multiple classes (multinomial regression), making it versatile for various classification problems.\n",
        "Feature Importance: It can interpret model coefficients as indicators of feature importance, revealing the direction and magnitude of the relationship between predictors and the outcome.\n",
        "Performance on Linearly Separable Data: Logistic regression performs well when the dataset is linearly separable, meaning that a straight line can separate the two classes.\n",
        "Disadvantages of Logistic Regression\n",
        "Assumption of Linearity: One of the main limitations of logistic regression is the assumption of linearity between the dependent variable and the independent variables. In real-world scenarios, data is rarely linearly separable, which can limit the model's effectiveness.\n",
        "Overfitting in High-Dimensional Data: Logistic regression can overfit in high-dimensional datasets, especially when the number of observations is smaller than the number of features. Regularization techniques (L1 and L2) are often used to mitigate this issue.\n",
        "Limited to Discrete Outcomes: Logistic regression is designed to predict discrete functions, meaning the dependent variable is restricted to a discrete number set. This makes it unsuitable for predicting continuous outcomes.\n",
        "Linear Decision Surface: Logistic regression constructs linear boundaries, which means it cannot effectively model non-linear relationships. Non-linear problems often require more complex models like neural networks.\n",
        "Multicollinearity Sensitivity: Logistic regression requires average or no multicollinearity between independent variables. High multicollinearity can lead to unstable and unreliable estimates.\n",
        "Inability to Capture Complex Relationships: Logistic regression is not well-suited for capturing complex relationships between variables. More powerful algorithms, such as neural networks, can often outperform it in such cases.\n",
        "Sensitivity to Outliers: Outliers can significantly impact the performance of logistic regression models, making it important to handle them carefully during preprocessing.\n",
        "In summary, logistic regression is a simple, efficient, and interpretable method for classification tasks, particularly when the data is linearly separable. However, it has limitations in handling non-linear relationships, high-dimensional data, and complex interactions between variables.\n",
        "\n",
        "Q17.What are some use cases of Logistic Regression?\n",
        "\n",
        "ANS.Logistic regression has several use cases across different domains. It is commonly used for prediction and classification problems. Some of these use cases include:\n",
        "\n",
        "Fraud detection: Logistic regression models can help teams identify data anomalies, which are predictive of fraud. This is particularly helpful to banking and other financial institutions in protecting their clients.\n",
        "Disease prediction: In medicine, logistic regression can be used to predict the likelihood of disease or illness for a given population. Healthcare organizations can set up preventive care for individuals that show a higher propensity for specific illnesses.\n",
        "Credit scoring: Logistic regression is widely used in credit scoring and shows remarkable results. It helps in predicting if a customer is going to default on their credit card based on characteristics such as annual income, monthly credit card payments, and the number of defaults.\n",
        "Customer churn prediction: Logistic regression can be used to predict whether a customer is likely to stop using a service or product. This helps companies to take proactive measures to retain customers.\n",
        "Marketing: Logistic regression is used in marketing to predict the probability of a customer responding to a promotional offer. This helps in optimizing marketing strategies and improving ROI.\n",
        "\n",
        "Q18.What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "ANS.Softmax regression and logistic regression are both used for classification tasks, but they differ in their application and functionality. Logistic regression is typically used for binary classification, where the goal is to predict one of two possible outcomes. In contrast, softmax regression is a generalization of logistic regression used for multi-class classification, where the goal is to predict one of more than two possible outcomes.\n",
        "\n",
        "The key difference lies in the output of the models. Logistic regression outputs a single probability value for the positive class, whereas softmax regression outputs a vector of probabilities for each class, ensuring that the sum of these probabilities equals 1.\n",
        " This makes softmax regression suitable for scenarios where the classes are mutually exclusive, meaning an instance can belong to only one class.\n",
        "\n",
        "Additionally, the cost function used in softmax regression is the cross-entropy loss, which is appropriate for multi-class problems, while logistic regression typically uses a binary cross-entropy loss.\n",
        " The softmax function, which is used in softmax regression, normalizes the output of a network to a probability distribution over predicted output classes, whereas the sigmoid function, used in logistic regression, maps the input to a value between 0 and 1.\n",
        "\n",
        " Q19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        " ANS.When choosing between One-vs-Rest (OvR) and Softmax for multiclass classification, it's important to consider the specific characteristics and requirements of your classification task.\n",
        "\n",
        "One-vs-Rest (OvR), also known as one-vs-all, involves training a separate binary classifier for each class. Each classifier is trained to distinguish one class from all the others. This method is straightforward and can be effective when the number of classes is relatively small. However, as the number of classes increases, the inefficiency of training and using multiple classifiers also increases. OvR can be advantageous for algorithms such as kernel methods which don't scale well with the number of samples, since each individual learning problem only involves a small subset of the data.\n",
        "\n",
        "On the other hand, Softmax is a function that converts a vector of K real values into a vector of K real values that sum to 1, effectively representing probabilities. In the context of multiclass classification, Softmax is used in multinomial logistic regression and neural networks to model the probability distribution over the classes. It is particularly useful when you want to predict the probabilities of each class relative to each other, ensuring that these probabilities sum to 1. This approach is more efficient than OvR when the number of classes is large, especially when using deep neural networks where each output node represents a different class.\n",
        "\n",
        "The choice between OvR and Softmax also depends on the nature of the problem. Softmax assumes that each example belongs to exactly one class, making it suitable for mutually exclusive class problems. If examples can belong to multiple classes simultaneously, multiple logistic regressions or other methods may be more appropriate.\n",
        "\n",
        "In summary, if the number of classes is small and the algorithm used scales poorly with the number of samples, OvR might be a better choice. However, for a larger number of classes or when the assumption of mutual exclusivity holds, Softmax provides a more efficient and probabilistically coherent approach.\n",
        "\n",
        "Q20.How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "ANS.In logistic regression, coefficients represent the change in the log odds of the outcome for a one-unit increase in the predictor variable, holding other variables constant.\n",
        " For example, if a coefficient for a predictor variable is 0.38, this means that a one-unit increase in that variable increases the log odds of the outcome by 0.38.\n",
        "\n",
        "To make these coefficients more interpretable, they are often converted into odds ratios by exponentiating the coefficients. An odds ratio greater than 1 indicates that the predictor increases the odds of the outcome, while an odds ratio less than 1 indicates a decrease in the odds of the outcome.\n",
        " For instance, if the coefficient for a predictor is 0.38, the odds ratio would be e\n",
        "0.38\n",
        " , which is approximately 1.46. This means that a one-unit increase in the predictor variable is associated with a 46% increase in the odds of the outcome.\n",
        "\n",
        "Additionally, the intercept in a logistic regression model represents the log odds of the outcome when all predictor variables are zero. For example, if the intercept is -1.93, the log odds of the outcome when all predictors are zero is -1.93.\n",
        "\n",
        "It is also important to note that the interpretation of coefficients can vary depending on the context and the specific model used. For instance, the effect of a predictor variable on the probability of the outcome can be non-linear and depends on the values of other variables in the model.\n",
        " Therefore, it is often useful to calculate marginal effects or use the logistic function to estimate the actual probabilities for specific values of the predictors."
      ],
      "metadata": {
        "id": "b3ijS1RwgHpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Reasoning (Practical)\n",
        "\n",
        "Q1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy.\n"
      ],
      "metadata": {
        "id": "xHmam8H5mV4g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2acb1bd",
        "outputId": "1e234d62-b764-4e17-aa22-e5b3e045d31f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_df = pd.DataFrame(X)\n",
        "y_df = pd.Series(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy."
      ],
      "metadata": {
        "id": "WTC7CcKynRoO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c24fdb5b",
        "outputId": "4ff24dac-afa3-4ae6-9d17-e2b917483f94"
      },
      "source": [
        "# Apply Logistic Regression with L1 regularization\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', random_state=42) # 'liblinear' solver is needed for L1 penalty\n",
        "model_l1.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_l1 = model_l1.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
        "print(f\"Model Accuracy with L1 regularization: {accuracy_l1}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 regularization: 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "b82Ql9M5nf6r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc6b585b",
        "outputId": "bc61e366-5672-4334-9eba-b68ff5f70a4d"
      },
      "source": [
        "# Apply Logistic Regression with L2 regularization\n",
        "model_l2 = LogisticRegression(penalty='l2', solver='lbfgs', random_state=42) # 'lbfgs' is the default solver and supports L2\n",
        "model_l2.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
        "print(f\"Model Accuracy with L2 regularization: {accuracy_l2}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients with L2 regularization:\")\n",
        "for i, coef in enumerate(model_l2.coef_[0]):\n",
        "    print(f\"Feature {i}: {coef}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 regularization: 0.855\n",
            "Model Coefficients with L2 regularization:\n",
            "Feature 0: 0.09060825018124448\n",
            "Feature 1: -0.49357666848011406\n",
            "Feature 2: 0.22859245027240999\n",
            "Feature 3: 0.10190401450485098\n",
            "Feature 4: -0.023276817357692003\n",
            "Feature 5: 1.6016124154050724\n",
            "Feature 6: -0.08237559436251198\n",
            "Feature 7: -0.01880023084306881\n",
            "Feature 8: -0.02388604084286504\n",
            "Feature 9: 0.009071231174687715\n",
            "Feature 10: 0.20610274355457728\n",
            "Feature 11: 0.4538921863796379\n",
            "Feature 12: 0.03163267971536173\n",
            "Feature 13: 0.13587083061537505\n",
            "Feature 14: -0.5078297597472073\n",
            "Feature 15: 0.07921165226269283\n",
            "Feature 16: 0.0703860772094714\n",
            "Feature 17: -0.15128687684651074\n",
            "Feature 18: -0.7585855064095278\n",
            "Feature 19: 0.10878210779513542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "r1m46cttDIXN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76472fb3",
        "outputId": "264f99c3-6f0d-4aa5-fc05-d7d1d265e66f"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "\n",
        "# Generate a synthetic dataset (assuming this is needed for context)\n",
        "# If X_train, y_train, X_test, y_test are already defined in previous cells, you can remove this.\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_df = pd.DataFrame(X)\n",
        "y_df = pd.Series(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Apply Logistic Regression with Elastic Net regularization\n",
        "# The 'saga' solver and 'l1_ratio' parameter are required for elasticnet\n",
        "model_elasticnet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=42)\n",
        "model_elasticnet.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_elasticnet = model_elasticnet.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_elasticnet = accuracy_score(y_test, y_pred_elasticnet)\n",
        "print(f\"Model Accuracy with Elastic Net regularization: {accuracy_elasticnet}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients with Elastic Net regularization:\")\n",
        "for i, coef in enumerate(model_elasticnet.coef_[0]):\n",
        "    print(f\"Feature {i}: {coef}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net regularization: 0.855\n",
            "Model Coefficients with Elastic Net regularization:\n",
            "Feature 0: 0.08556171047133405\n",
            "Feature 1: -0.009241809651199176\n",
            "Feature 2: 0.22315807173755084\n",
            "Feature 3: 0.0955667049859451\n",
            "Feature 4: -0.01752974389472479\n",
            "Feature 5: 1.9510766730632336\n",
            "Feature 6: -0.07802263050073019\n",
            "Feature 7: -0.012419785789264996\n",
            "Feature 8: -0.016602636424090832\n",
            "Feature 9: 0.004300085759822035\n",
            "Feature 10: 0.20030582195510283\n",
            "Feature 11: 0.4495411746171237\n",
            "Feature 12: 0.026060578303053365\n",
            "Feature 13: 0.12995661686440732\n",
            "Feature 14: -0.25573486001981366\n",
            "Feature 15: 0.07133958721566429\n",
            "Feature 16: 0.06418397543675146\n",
            "Feature 17: -0.14471945782101575\n",
            "Feature 18: -0.4921607001562043\n",
            "Feature 19: 0.10415396772678372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'."
      ],
      "metadata": {
        "id": "UpIJVH6wD_gi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e879885",
        "outputId": "294c0db2-5abb-4fb7-fbd1-da46cefd7f3c"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined from previous cells.\n",
        "# If not, you would need to generate or load a multiclass dataset and split it here.\n",
        "\n",
        "# Apply Logistic Regression for multiclass classification using 'ovr'\n",
        "# For 'ovr', the solver doesn't have to be 'saga' as it's a strategy, not a solver itself.\n",
        "# 'lbfgs' is a good default solver for 'ovr' with L2 penalty (default).\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='lbfgs', random_state=42)\n",
        "model_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_ovr = model_ovr.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Model Accuracy with One-vs-Rest (OvR): {accuracy_ovr}\")\n",
        "\n",
        "# Note: For multiclass, the coefficients are per class.\n",
        "# print(\"Model Coefficients with OvR:\")\n",
        "# print(model_ovr.coef_)\n",
        "# print(\"Model Intercepts with OvR:\")\n",
        "# print(model_ovr.intercept_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest (OvR): 0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "KNLNjuPFERjm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d29dc81",
        "outputId": "52e36390-9ed4-4b85-d6d5-9c53c33f037c"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'penalty': ['l1', 'l2'] # Type of regularization\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "# Note: 'liblinear' solver supports both 'l1' and 'l2' penalties, which is suitable for this grid search.\n",
        "# For 'l1' penalty, other solvers like 'lbfgs', 'sag', 'saga' and 'newton-cg' may not work by default.\n",
        "# 'saga' solver supports 'elasticnet' as well.\n",
        "# We will use 'liblinear' as it's simple and supports both L1 and L2 for this example.\n",
        "model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Cross-validation Accuracy: {best_accuracy}\")\n",
        "\n",
        "# Optionally, evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Test Set Accuracy with Best Parameters: {test_accuracy}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l1'}\n",
            "Best Cross-validation Accuracy: 0.875\n",
            "Test Set Accuracy with Best Parameters: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "p-vNJd0xFCJI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0849ccc3",
        "outputId": "c6383741-ce1e-42e0-efc6-387ce5305639"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_df and y_df are already defined from previous cells.\n",
        "# If not, you would need to generate or load a dataset here.\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create Stratified K-Fold Cross-Validation\n",
        "# n_splits is the number of folds\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy for each fold\n",
        "accuracy_scores = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X_df, y_df):\n",
        "    X_train_fold, X_test_fold = X_df.iloc[train_index], X_df.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y_df.iloc[train_index], y_df.iloc[test_index]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Predict on the test fold\n",
        "    y_pred_fold = model.predict(X_test_fold)\n",
        "\n",
        "    # Calculate accuracy for the current fold\n",
        "    accuracy_fold = accuracy_score(y_test_fold, y_pred_fold)\n",
        "    accuracy_scores.append(accuracy_fold)\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "print(f\"Average Accuracy using Stratified K-Fold Cross-Validation: {average_accuracy}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy using Stratified K-Fold Cross-Validation: 0.869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "0QOKKBR2FbGE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQZzcqDTFgmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "f9b30a34",
        "outputId": "06dcaca6-f4ab-4a6e-9512-abbb137952a7"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "# Define the path to the CSV file.\n",
        "# You'll need to upload your CSV file to this path or change the path accordingly.\n",
        "csv_file_path = '/content/your_dataset.csv' # Replace with the actual path to your CSV file\n",
        "\n",
        "# --- Create a dummy CSV file for demonstration if it doesn't exist ---\n",
        "# In a real scenario, you would skip this part and use your own CSV.\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"Creating a dummy CSV file at {csv_file_path} for demonstration.\")\n",
        "    # Create a simple dummy dataset\n",
        "    data = {\n",
        "        'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # Binary target variable\n",
        "    }\n",
        "    dummy_df = pd.DataFrame(data)\n",
        "    dummy_df.to_csv(csv_file_path, index=False)\n",
        "    print(\"Dummy CSV file created.\")\n",
        "# --- End of dummy CSV creation ---\n",
        "\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "    print(f\"Dataset loaded successfully from {csv_file_path}\")\n",
        "    print(\"First 5 rows of the dataset:\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
        "    # You might want to stop execution or handle this error appropriately\n",
        "    # For this example, we'll exit if the file is not found\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the CSV file: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Assume the last column is the target variable and the rest are features\n",
        "# You may need to adjust this based on your dataset\n",
        "X = df.drop(columns=[df.columns[-1]]) # Features\n",
        "y = df[df.columns[-1]] # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nTraining and testing sets created.\")\n",
        "print(f\"Training set shape (X_train): {X_train.shape}\")\n",
        "print(f\"Testing set shape (X_test): {X_test.shape}\")\n",
        "print(f\"Training set shape (y_train): {y_train.shape}\")\n",
        "print(f\"Testing set shape (y_test): {y_test.shape}\")\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nLogistic Regression model trained.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy on the test set: {accuracy}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a dummy CSV file at /content/your_dataset.csv for demonstration.\n",
            "Dummy CSV file created.\n",
            "Dataset loaded successfully from /content/your_dataset.csv\n",
            "First 5 rows of the dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   feature1  feature2  target\n",
              "0         1        10       0\n",
              "1         2         9       0\n",
              "2         3         8       0\n",
              "3         4         7       0\n",
              "4         5         6       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95c0c14b-7c57-4238-aeaa-93e272f40dea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95c0c14b-7c57-4238-aeaa-93e272f40dea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95c0c14b-7c57-4238-aeaa-93e272f40dea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95c0c14b-7c57-4238-aeaa-93e272f40dea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-70cdc3d8-6060-4b21-bb38-cff8f1f17a79\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-70cdc3d8-6060-4b21-bb38-cff8f1f17a79')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-70cdc3d8-6060-4b21-bb38-cff8f1f17a79 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"\\\\nModel Accuracy on the test set: {accuracy}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 6,\n        \"max\": 10,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9,\n          6,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and testing sets created.\n",
            "Training set shape (X_train): (8, 2)\n",
            "Testing set shape (X_test): (2, 2)\n",
            "Training set shape (y_train): (8,)\n",
            "Testing set shape (y_test): (2,)\n",
            "\n",
            "Logistic Regression model trained.\n",
            "\n",
            "Model Accuracy on the test set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "Rc8DKHyYFptz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab280252",
        "outputId": "1cd692f6-dcf9-41e7-bf2a-e85501f3f685"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, y_train are already defined from previous cells.\n",
        "# If not, you would need to generate or load a dataset and split it here.\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Regularization parameter (log scale)\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None], # Type of regularization\n",
        "    'solver': ['liblinear', 'lbfgs', 'sag', 'saga'] # Solver for optimization\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "# We'll use a solver that supports multiple penalties for flexibility in tuning.\n",
        "# 'saga' is a good choice as it supports 'l1', 'l2', 'elasticnet', and 'None'.\n",
        "# We need to handle the 'elasticnet' penalty which requires 'l1_ratio'.\n",
        "# For simplicity in this example, we'll include 'elasticnet' in the penalty list,\n",
        "# but in a more complex scenario, you might want to handle 'l1_ratio' tuning as well,\n",
        "# or restrict penalties based on the chosen solver.\n",
        "# Note: Not all penalty/solver combinations are compatible. RandomizedSearchCV will\n",
        "# skip incompatible combinations.\n",
        "model = LogisticRegression(random_state=42, max_iter=1000) # Increased max_iter for convergence\n",
        "\n",
        "# Create RandomizedSearchCV object\n",
        "# n_iter is the number of parameter settings that are sampled.\n",
        "# cv is the number of cross-validation folds.\n",
        "# scoring is the evaluation metric.\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "print(\"Starting RandomizedSearchCV...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"RandomizedSearchCV finished.\")\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_accuracy = random_search.best_score_\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"\\nBest Parameters from RandomizedSearchCV: {best_params}\")\n",
        "print(f\"Best Cross-validation Accuracy from RandomizedSearchCV: {best_accuracy}\")\n",
        "\n",
        "# Optionally, evaluate the best model on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Test Set Accuracy with Best Parameters: {test_accuracy}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting RandomizedSearchCV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomizedSearchCV finished.\n",
            "\n",
            "Best Parameters from RandomizedSearchCV: {'solver': 'sag', 'penalty': 'l2', 'C': np.float64(78.47599703514607)}\n",
            "Best Cross-validation Accuracy from RandomizedSearchCV: 0.8\n",
            "Test Set Accuracy with Best Parameters: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.7 0.8 nan 0.8 nan 0.8 nan 0.8 0.8 0.7]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "BLjAKYyBF_B-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd4b2499",
        "outputId": "c51dc28f-862c-4944-8071-7558dd7ca51d"
      },
      "source": [
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined from previous cells,\n",
        "# and that y_train contains more than two unique classes for multiclass classification.\n",
        "# If not, you would need to generate or load a multiclass dataset and split it here.\n",
        "# For demonstration, let's check if the target variable is multiclass.\n",
        "# If the previous dataset was binary, we'll generate a new multiclass one for this example.\n",
        "\n",
        "if len(y_df.unique()) <= 2:\n",
        "    print(\"Generating a new multiclass dataset for demonstration...\")\n",
        "    from sklearn.datasets import make_classification\n",
        "    # Generate a synthetic multiclass dataset\n",
        "    X_multi, y_multi = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, n_clusters_per_class=2, random_state=42)\n",
        "    X_multi_df = pd.DataFrame(X_multi)\n",
        "    y_multi_df = pd.Series(y_multi)\n",
        "\n",
        "    # Split the multiclass dataset\n",
        "    X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_multi_df, y_multi_df, test_size=0.2, random_state=42, stratify=y_multi_df)\n",
        "    print(\"Multiclass dataset generated and split.\")\n",
        "else:\n",
        "    print(\"Using the existing multiclass dataset.\")\n",
        "    X_train_multi, X_test_multi, y_train_multi, y_test_multi = X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# Create a Logistic Regression base model\n",
        "base_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Apply One-vs-One (OvO) multiclass strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Fit the OvO model to the training data\n",
        "print(\"Fitting One-vs-One model...\")\n",
        "# Use the appropriate training data based on whether a new dataset was generated\n",
        "if 'X_train_multi' in locals():\n",
        "    ovo_model.fit(X_train_multi, y_train_multi)\n",
        "else:\n",
        "    ovo_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"One-vs-One model fitted.\")\n",
        "\n",
        "# Predict on the test set\n",
        "# Use the appropriate testing data\n",
        "if 'X_test_multi' in locals():\n",
        "    y_pred_ovo = ovo_model.predict(X_test_multi)\n",
        "    y_test_actual = y_test_multi\n",
        "else:\n",
        "    y_pred_ovo = ovo_model.predict(X_test)\n",
        "    y_test_actual = y_test\n",
        "\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_ovo = accuracy_score(y_test_actual, y_pred_ovo)\n",
        "print(f\"\\nModel Accuracy with One-vs-One (OvO): {accuracy_ovo}\")\n",
        "\n",
        "# You can also inspect the estimators (the binary classifiers) trained by OvO\n",
        "# print(\"\\nNumber of estimators (binary classifiers) trained:\", len(ovo_model.estimators_))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating a new multiclass dataset for demonstration...\n",
            "Multiclass dataset generated and split.\n",
            "Fitting One-vs-One model...\n",
            "One-vs-One model fitted.\n",
            "\n",
            "Model Accuracy with One-vs-One (OvO): 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "jcAx34gdGgm7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "5d0455af",
        "outputId": "1000b5fb-cc85-4520-f893-ece8805effb0"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Load or generate a binary dataset ---\n",
        "# We'll use the dummy CSV created in a previous cell if it exists.\n",
        "# If not, we'll create one or you can load your own binary dataset.\n",
        "csv_file_path = '/content/your_dataset.csv'\n",
        "\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"Creating a dummy binary CSV file at {csv_file_path} for demonstration.\")\n",
        "    data = {\n",
        "        'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # Binary target variable\n",
        "    }\n",
        "    dummy_df = pd.DataFrame(data)\n",
        "    dummy_df.to_csv(csv_file_path, index=False)\n",
        "    print(\"Dummy binary CSV file created.\")\n",
        "\n",
        "try:\n",
        "    df_binary = pd.read_csv(csv_file_path)\n",
        "    print(f\"Binary dataset loaded successfully from {csv_file_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file_path}' was not found. Please upload your binary classification CSV file.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the CSV file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Assume the last column is the target variable and the rest are features\n",
        "# You may need to adjust this based on your dataset\n",
        "X_binary = df_binary.drop(columns=[df_binary.columns[-1]]) # Features\n",
        "y_binary = df_binary[df_binary.columns[-1]] # Target variable\n",
        "\n",
        "# Check if the dataset is indeed binary\n",
        "if len(y_binary.unique()) != 2:\n",
        "    print(\"Error: The loaded dataset does not appear to be for binary classification.\")\n",
        "    print(\"Please provide a dataset with exactly two unique values in the target column.\")\n",
        "    exit()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
        "\n",
        "print(\"\\nBinary training and testing sets created.\")\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model_binary = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_binary.fit(X_train_binary, y_train_binary)\n",
        "\n",
        "print(\"Binary Logistic Regression model trained.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_binary = model_binary.predict(X_test_binary)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test_binary, y_pred_binary, labels=model_binary.classes_)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_binary.classes_)\n",
        "disp.plot()\n",
        "\n",
        "plt.title('Confusion Matrix for Binary Logistic Regression')\n",
        "plt.show()\n",
        "\n",
        "# Optionally, print classification report for more metrics\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_binary, y_pred_binary))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary dataset loaded successfully from /content/your_dataset.csv\n",
            "\n",
            "Binary training and testing sets created.\n",
            "Binary Logistic Regression model trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "uKJ7q-hEGzrf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "826a5e80",
        "outputId": "95d46271-f0bc-4508-a44d-e70864fb28d3"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "# Assuming X_train_binary, X_test_binary, y_train_binary, and y_test_binary\n",
        "# are already defined from the previous cell with a binary dataset.\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model_eval = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_eval.fit(X_train_binary, y_train_binary)\n",
        "\n",
        "print(\"Logistic Regression model trained for evaluation.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_eval = model_eval.predict(X_test_binary)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test_binary, y_pred_eval)\n",
        "recall = recall_score(y_test_binary, y_pred_eval)\n",
        "f1 = f1_score(y_test_binary, y_pred_eval)\n",
        "\n",
        "print(f\"\\nPrecision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "\n",
        "# Optionally, print a full classification report for more detailed metrics\n",
        "print(\"\\nFull Classification Report:\")\n",
        "print(classification_report(y_test_binary, y_pred_eval))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained for evaluation.\n",
            "\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "\n",
            "Full Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "m6shpFncHKpg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2b109ea",
        "outputId": "76aa0905-437b-4d31-a712-53c68f16c4fb"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# --- Generate an imbalanced dataset ---\n",
        "# We'll create a dataset with a skewed class distribution.\n",
        "X_imb, y_imb = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                                   n_redundant=5, weights=[0.95, 0.05], flip_y=0, random_state=42)\n",
        "\n",
        "X_imb_df = pd.DataFrame(X_imb)\n",
        "y_imb_series = pd.Series(y_imb)\n",
        "\n",
        "print(\"Generated imbalanced dataset.\")\n",
        "print(\"Class distribution:\")\n",
        "print(y_imb_series.value_counts())\n",
        "\n",
        "# Split the imbalanced dataset into training and testing sets\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(X_imb_df, y_imb_series,\n",
        "                                                                    test_size=0.2, random_state=42,\n",
        "                                                                    stratify=y_imb_series) # Stratify to maintain class distribution in splits\n",
        "\n",
        "print(\"\\nImbalanced training and testing sets created.\")\n",
        "print(f\"Training set shape (X_train_imb): {X_train_imb.shape}\")\n",
        "print(f\"Testing set shape (X_test_imb): {X_test_imb.shape}\")\n",
        "\n",
        "# --- Train Logistic Regression without class weights ---\n",
        "print(\"\\nTraining Logistic Regression model WITHOUT class weights...\")\n",
        "model_no_weights = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear for simplicity\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_no_weights.fit(X_train_imb, y_train_imb)\n",
        "\n",
        "print(\"Model without class weights trained.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_no_weights = model_no_weights.predict(X_test_imb)\n",
        "\n",
        "# Evaluate the model without class weights\n",
        "print(\"\\nClassification Report (Without Class Weights):\")\n",
        "print(classification_report(y_test_imb, y_pred_no_weights))\n",
        "\n",
        "# --- Train Logistic Regression WITH class weights ---\n",
        "print(\"\\nTraining Logistic Regression model WITH class weights...\")\n",
        "# The 'balanced' option automatically adjusts weights inversely proportional to class frequencies\n",
        "model_with_weights = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_with_weights.fit(X_train_imb, y_train_imb)\n",
        "\n",
        "print(\"Model with class weights trained.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_with_weights = model_with_weights.predict(X_test_imb)\n",
        "\n",
        "# Evaluate the model with class weights\n",
        "print(\"\\nClassification Report (With Class Weights):\")\n",
        "print(classification_report(y_test_imb, y_pred_with_weights))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated imbalanced dataset.\n",
            "Class distribution:\n",
            "0    950\n",
            "1     50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Imbalanced training and testing sets created.\n",
            "Training set shape (X_train_imb): (800, 20)\n",
            "Testing set shape (X_test_imb): (200, 20)\n",
            "\n",
            "Training Logistic Regression model WITHOUT class weights...\n",
            "Model without class weights trained.\n",
            "\n",
            "Classification Report (Without Class Weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97       190\n",
            "           1       0.50      0.10      0.17        10\n",
            "\n",
            "    accuracy                           0.95       200\n",
            "   macro avg       0.73      0.55      0.57       200\n",
            "weighted avg       0.93      0.95      0.93       200\n",
            "\n",
            "\n",
            "Training Logistic Regression model WITH class weights...\n",
            "Model with class weights trained.\n",
            "\n",
            "Classification Report (With Class Weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.78      0.87       190\n",
            "           1       0.11      0.50      0.18        10\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.54      0.64      0.52       200\n",
            "weighted avg       0.92      0.77      0.83       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "H9-G8KyzHnvN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "8d0ca204",
        "outputId": "b0742ac8-191e-44fa-8ee3-f7d4b0f793e4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the URL of the Titanic dataset\n",
        "titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "\n",
        "# Load the dataset from the URL into a DataFrame\n",
        "titanic_df = pd.read_csv(titanic_url)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"First 5 rows of the Titanic dataset:\")\n",
        "display(titanic_df.head())\n",
        "\n",
        "# Display information about the DataFrame to see column types and non-null counts\n",
        "print(\"\\nInformation about the Titanic dataset:\")\n",
        "display(titanic_df.info())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the Titanic dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  \n",
              "4      0            373450   8.0500   NaN        S  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b0bb7ac-783c-4199-95ff-084bc301c255\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b0bb7ac-783c-4199-95ff-084bc301c255')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2b0bb7ac-783c-4199-95ff-084bc301c255 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2b0bb7ac-783c-4199-95ff-084bc301c255');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-33bc2629-f9c5-4d61-bbb8-acce05606f94\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-33bc2629-f9c5-4d61-bbb8-acce05606f94')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-33bc2629-f9c5-4d61-bbb8-acce05606f94 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(titanic_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"PassengerId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",\n          \"Allen, Mr. William Henry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\",\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.833739825307955,\n        \"min\": 22.0,\n        \"max\": 38.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          38.0,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SibSp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ticket\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"PC 17599\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.5100288352535,\n        \"min\": 7.25,\n        \"max\": 71.2833,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          71.2833\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cabin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"C123\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Information about the Titanic dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8080b0a3"
      },
      "source": [
        "## Handle missing values\n",
        "\n",
        "### Subtask:\n",
        "Identify and handle missing values in the dataset by imputing or removing them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "654806af"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify missing values and handle them by imputing 'Age' and 'Embarked' and dropping 'Cabin'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "id": "7a6d8fa7",
        "outputId": "6a7deeca-527f-46e4-93c5-730a2dc54faf"
      },
      "source": [
        "# Identify columns with missing values\n",
        "print(\"Missing values before handling:\")\n",
        "display(titanic_df.isnull().sum())\n",
        "\n",
        "# Handle missing values\n",
        "# Impute missing 'Age' values with the median\n",
        "median_age = titanic_df['Age'].median()\n",
        "titanic_df['Age'].fillna(median_age, inplace=True)\n",
        "\n",
        "# Impute missing 'Embarked' values with the mode\n",
        "mode_embarked = titanic_df['Embarked'].mode()[0]\n",
        "titanic_df['Embarked'].fillna(mode_embarked, inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column due to a large number of missing values\n",
        "titanic_df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Verify that there are no remaining missing values in the relevant columns\n",
        "print(\"\\nMissing values after handling:\")\n",
        "display(titanic_df.isnull().sum())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before handling:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age            177\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         2\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PassengerId</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Survived</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pclass</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sex</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SibSp</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Parch</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ticket</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fare</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cabin</th>\n",
              "      <td>687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Embarked</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values after handling:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "PassengerId    0\n",
              "Survived       0\n",
              "Pclass         0\n",
              "Name           0\n",
              "Sex            0\n",
              "Age            0\n",
              "SibSp          0\n",
              "Parch          0\n",
              "Ticket         0\n",
              "Fare           0\n",
              "Embarked       0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PassengerId</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Survived</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pclass</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sex</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SibSp</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Parch</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ticket</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fare</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Embarked</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "JdxnAuqpIUf6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "c75e40d8",
        "outputId": "824c6fe5-9c6a-4f65-f74e-cb72cd9f1228"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'titanic_df' is already loaded and missing values are handled from previous cells.\n",
        "# If not, you would need to load the dataset and handle missing values here.\n",
        "\n",
        "# --- Feature Selection ---\n",
        "# Select features and the target variable\n",
        "# Dropping 'PassengerId', 'Name', 'Ticket' as they are not typically used as features\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = titanic_df[features]\n",
        "y = titanic_df[target]\n",
        "\n",
        "print(\"Selected features and target variable.\")\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "\n",
        "# --- Handle Categorical Features ---\n",
        "# Convert categorical features into dummy variables (One-Hot Encoding)\n",
        "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True) # drop_first=True to avoid multicollinearity\n",
        "\n",
        "print(\"\\nHandled categorical features (One-Hot Encoding).\")\n",
        "print(f\"Features (X) shape after encoding: {X.shape}\")\n",
        "display(X.head())\n",
        "\n",
        "# --- Split the dataset ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"\\nDataset split into training and testing sets.\")\n",
        "print(f\"Training set shape (X_train): {X_train.shape}\")\n",
        "print(f\"Testing set shape (X_test): {X_test.shape}\")\n",
        "\n",
        "# --- Train Logistic Regression WITHOUT Scaling ---\n",
        "print(\"\\nTraining Logistic Regression model WITHOUT scaling...\")\n",
        "model_no_scale = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear for simplicity and compatibility\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model without scaling trained.\")\n",
        "\n",
        "# Predict and evaluate without scaling\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "print(f\"Model Accuracy WITHOUT scaling: {accuracy_no_scale}\")\n",
        "\n",
        "# --- Apply Feature Scaling (Standardization) ---\n",
        "print(\"\\nApplying Feature Scaling (Standardization)...\")\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling applied.\")\n",
        "print(f\"Scaled Training set shape (X_train_scaled): {X_train_scaled.shape}\")\n",
        "print(f\"Scaled Testing set shape (X_test_scaled): {X_test_scaled.shape}\")\n",
        "\n",
        "\n",
        "# --- Train Logistic Regression WITH Scaling ---\n",
        "print(\"\\nTraining Logistic Regression model WITH scaling...\")\n",
        "# Using a solver that performs well with scaled data, like 'lbfgs' or 'saga'\n",
        "model_with_scale = LogisticRegression(random_state=42, solver='lbfgs', max_iter=1000)\n",
        "model_with_scale.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Model with scaling trained.\")\n",
        "\n",
        "# Predict and evaluate with scaling\n",
        "y_pred_with_scale = model_with_scale.predict(X_test_scaled)\n",
        "accuracy_with_scale = accuracy_score(y_test, y_pred_with_scale)\n",
        "\n",
        "print(f\"Model Accuracy WITH scaling: {accuracy_with_scale}\")\n",
        "\n",
        "# --- Compare Results ---\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scale}\")\n",
        "\n",
        "if accuracy_with_scale > accuracy_no_scale:\n",
        "    print(\"Applying scaling improved the model accuracy.\")\n",
        "elif accuracy_with_scale < accuracy_no_scale:\n",
        "    print(\"Applying scaling decreased the model accuracy.\")\n",
        "else:\n",
        "    print(\"Applying scaling did not change the model accuracy.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features and target variable.\n",
            "Features (X) shape: (891, 7)\n",
            "Target (y) shape: (891,)\n",
            "\n",
            "Handled categorical features (One-Hot Encoding).\n",
            "Features (X) shape after encoding: (891, 8)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
              "0       3  22.0      1      0   7.2500      True       False        True\n",
              "1       1  38.0      1      0  71.2833     False       False       False\n",
              "2       3  26.0      0      0   7.9250     False       False        True\n",
              "3       1  35.0      1      0  53.1000     False       False        True\n",
              "4       3  35.0      0      0   8.0500      True       False        True"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62aaa4cd-9ff0-4774-b21e-93f9ae58a6e7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62aaa4cd-9ff0-4774-b21e-93f9ae58a6e7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62aaa4cd-9ff0-4774-b21e-93f9ae58a6e7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62aaa4cd-9ff0-4774-b21e-93f9ae58a6e7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8d709a91-a84d-445c-b016-73b7b078a5db\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d709a91-a84d-445c-b016-73b7b078a5db')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8d709a91-a84d-445c-b016-73b7b078a5db button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"Applying scaling did not change the model accuracy\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.833739825307955,\n        \"min\": 22.0,\n        \"max\": 38.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          38.0,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SibSp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.5100288352535,\n        \"min\": 7.25,\n        \"max\": 71.2833,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          71.2833\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex_male\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked_Q\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked_S\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset split into training and testing sets.\n",
            "Training set shape (X_train): (712, 8)\n",
            "Testing set shape (X_test): (179, 8)\n",
            "\n",
            "Training Logistic Regression model WITHOUT scaling...\n",
            "Model without scaling trained.\n",
            "Model Accuracy WITHOUT scaling: 0.8044692737430168\n",
            "\n",
            "Applying Feature Scaling (Standardization)...\n",
            "Feature scaling applied.\n",
            "Scaled Training set shape (X_train_scaled): (712, 8)\n",
            "Scaled Testing set shape (X_test_scaled): (179, 8)\n",
            "\n",
            "Training Logistic Regression model WITH scaling...\n",
            "Model with scaling trained.\n",
            "Model Accuracy WITH scaling: 0.8044692737430168\n",
            "\n",
            "--- Comparison ---\n",
            "Accuracy without scaling: 0.8044692737430168\n",
            "Accuracy with scaling: 0.8044692737430168\n",
            "Applying scaling did not change the model accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "Uc2DdNPLIosB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "60a9e49d",
        "outputId": "46648f6f-b2ac-4eae-e280-b2194d138474"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# If not, you would need to load, preprocess (handle missing values, encode categoricals), and split the data here.\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model_roc = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear for simplicity\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_roc.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained for ROC-AUC evaluation.\")\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "# roc_auc_score requires probabilities for the positive class\n",
        "y_prob_roc = model_roc.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob_roc)\n",
        "\n",
        "print(f\"\\nROC-AUC Score: {roc_auc}\")\n",
        "\n",
        "# Optionally, plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob_roc)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained for ROC-AUC evaluation.\n",
            "\n",
            "ROC-AUC Score: 0.8378129117259552\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "LEsaFcxiI2C3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f235397b",
        "outputId": "d6cbcffb-3c8c-4cc8-ac13-0efd3f6f36a8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset after handling missing values and encoding.\n",
        "\n",
        "# Create a Logistic Regression model with a custom C value (regularization strength)\n",
        "# C is the inverse of regularization strength; smaller values specify stronger regularization.\n",
        "# Using a solver that works well with the preprocessed data, like 'lbfgs' or 'liblinear'\n",
        "model_custom_c = LogisticRegression(C=0.5, random_state=42, solver='liblinear')\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_custom_c.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Logistic Regression model trained with C={model_custom_c.C}\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_custom_c = model_custom_c.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy_custom_c = accuracy_score(y_test, y_pred_custom_c)\n",
        "print(f\"Model Accuracy with C={model_custom_c.C}: {accuracy_custom_c}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained with C=0.5\n",
            "Model Accuracy with C=0.5: 0.7988826815642458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "3CsWhWxfJH5X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "83fa7700",
        "outputId": "9e9f8813-9e0a-4f82-8559-5360504e20a9"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X and y are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset\n",
        "# after handling missing values and encoding categorical variables.\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "# Using a solver that works well with the preprocessed data\n",
        "model_features = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# Fit the model to the entire dataset (or you could fit on the training set X_train, y_train)\n",
        "# For demonstrating feature importance based on coefficients, fitting on the full dataset is common.\n",
        "# If you need to use the test set for evaluation later, fit on X_train, y_train.\n",
        "model_features.fit(X, y)\n",
        "\n",
        "print(\"Logistic Regression model trained to identify feature importance.\")\n",
        "\n",
        "# Get the coefficients and feature names\n",
        "coefficients = model_features.coef_[0]\n",
        "feature_names = X.columns\n",
        "\n",
        "# Create a DataFrame to display coefficients with their corresponding feature names\n",
        "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort features by the absolute value of their coefficients to identify important ones\n",
        "coef_df['Absolute_Coefficient'] = abs(coef_df['Coefficient'])\n",
        "sorted_coef_df = coef_df.sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance based on Logistic Regression Coefficients:\")\n",
        "display(sorted_coef_df)\n",
        "\n",
        "# Note: The interpretation of coefficients assumes features are on a similar scale.\n",
        "# If you did not scale the features in previous steps, features with larger magnitudes\n",
        "# might appear more important due to their scale, not their actual impact.\n",
        "# Scaling features (like in Q15) makes coefficient comparison more meaningful.\n",
        "\n",
        "# For example, to see the sign of the coefficient (positive or negative impact on the log-odds of survival)\n",
        "# print(\"\\nFeature Coefficients (sorted by absolute value):\")\n",
        "# display(sorted_coef_df[['Feature', 'Coefficient']])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained to identify feature importance.\n",
            "\n",
            "Feature Importance based on Logistic Regression Coefficients:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      Feature  Coefficient  Absolute_Coefficient\n",
              "5    Sex_male    -2.477785              2.477785\n",
              "0      Pclass    -0.836298              0.836298\n",
              "2       SibSp    -0.293164              0.293164\n",
              "7  Embarked_S    -0.243733              0.243733\n",
              "3       Parch    -0.075543              0.075543\n",
              "6  Embarked_Q     0.037796              0.037796\n",
              "1         Age    -0.028051              0.028051\n",
              "4        Fare     0.004584              0.004584"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab95269e-7c01-4226-92af-ec05bf5e25ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Coefficient</th>\n",
              "      <th>Absolute_Coefficient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Sex_male</td>\n",
              "      <td>-2.477785</td>\n",
              "      <td>2.477785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pclass</td>\n",
              "      <td>-0.836298</td>\n",
              "      <td>0.836298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SibSp</td>\n",
              "      <td>-0.293164</td>\n",
              "      <td>0.293164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Embarked_S</td>\n",
              "      <td>-0.243733</td>\n",
              "      <td>0.243733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parch</td>\n",
              "      <td>-0.075543</td>\n",
              "      <td>0.075543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Embarked_Q</td>\n",
              "      <td>0.037796</td>\n",
              "      <td>0.037796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Age</td>\n",
              "      <td>-0.028051</td>\n",
              "      <td>0.028051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fare</td>\n",
              "      <td>0.004584</td>\n",
              "      <td>0.004584</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab95269e-7c01-4226-92af-ec05bf5e25ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab95269e-7c01-4226-92af-ec05bf5e25ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab95269e-7c01-4226-92af-ec05bf5e25ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8c3e8b70-23fe-4c45-bdf6-9ae3cc27d1b3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c3e8b70-23fe-4c45-bdf6-9ae3cc27d1b3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8c3e8b70-23fe-4c45-bdf6-9ae3cc27d1b3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3dff3b97-410b-4828-b8ff-f46d43bf7151\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sorted_coef_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3dff3b97-410b-4828-b8ff-f46d43bf7151 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('sorted_coef_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sorted_coef_df",
              "summary": "{\n  \"name\": \"sorted_coef_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Pclass\",\n          \"Embarked_Q\",\n          \"Sex_male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Coefficient\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8518004940748867,\n        \"min\": -2.4777852023101063,\n        \"max\": 0.0377955033725962,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          -0.8362976099710732,\n          0.0377955033725962,\n          -2.4777852023101063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Absolute_Coefficient\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.844744510021833,\n        \"min\": 0.004583554826376479,\n        \"max\": 2.4777852023101063,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8362976099710732,\n          0.0377955033725962,\n          2.4777852023101063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "b_WdgIApJUKI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "953af427",
        "outputId": "bb623346-3766-4208-c716-8f4ad0adbcca"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset after handling missing values and encoding.\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model_kappa = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_kappa.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained for Cohen's Kappa evaluation.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_kappa = model_kappa.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred_kappa)\n",
        "\n",
        "print(f\"\\nCohen's Kappa Score: {kappa_score}\")\n",
        "\n",
        "# Interpretation of Cohen's Kappa:\n",
        "# < 0    : Less than chance agreement\n",
        "# 0 – 0.20 : Slight agreement\n",
        "# 0.21 – 0.40 : Fair agreement\n",
        "# 0.41 – 0.60 : Moderate agreement\n",
        "# 0.61 – 0.80 : Substantial agreement\n",
        "# 0.81 – 1.00 : Almost perfect agreement"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained for Cohen's Kappa evaluation.\n",
            "\n",
            "Cohen's Kappa Score: 0.5746486523185552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "icY6YR1cJl4R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "148e63bf",
        "outputId": "4be63b1c-b540-4014-d5ca-2244e86f6dc0"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset after handling missing values and encoding.\n",
        "\n",
        "# If you are running this cell independently, make sure X_train, X_test, y_train, y_test are defined.\n",
        "# For example, you can uncomment and run the data loading and preprocessing steps from Q15 here:\n",
        "# # Define the URL of the Titanic dataset\n",
        "# titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "# # Load the dataset\n",
        "# titanic_df = pd.read_csv(titanic_url)\n",
        "# # Handle missing values (using median for Age, mode for Embarked, drop Cabin)\n",
        "# titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)\n",
        "# titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0], inplace=True)\n",
        "# titanic_df.drop(columns=['Cabin'], inplace=True)\n",
        "# # Select features and target\n",
        "# features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "# target = 'Survived'\n",
        "# X = titanic_df[features]\n",
        "# y = titanic_df[target]\n",
        "# # Handle Categorical Features\n",
        "# X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "# # Split the dataset\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model_pr_curve = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_pr_curve.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained for Precision-Recall Curve visualization.\")\n",
        "\n",
        "# Predict probabilities on the test set for the positive class (class 1)\n",
        "y_prob_pr_curve = model_pr_curve.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob_pr_curve)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained for Precision-Recall Curve visualization.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "Kc6Vlu5iJykv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dc69ade",
        "outputId": "9eab6d26-b534-4858-937f-1fb9ccf97eda"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset after handling missing values and encoding.\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers_to_compare = ['liblinear', 'lbfgs', 'saga']\n",
        "\n",
        "# Dictionary to store accuracy for each solver\n",
        "accuracy_by_solver = {}\n",
        "\n",
        "print(\"Training Logistic Regression models with different solvers...\")\n",
        "\n",
        "for solver in solvers_to_compare:\n",
        "    print(f\"\\nTraining with solver: {solver}\")\n",
        "    try:\n",
        "        # Create a Logistic Regression model with the current solver\n",
        "        # Note: 'liblinear' only supports 'l1' and 'l2' penalties.\n",
        "        # 'lbfgs' supports 'l2' and 'None' penalties.\n",
        "        # 'saga' supports 'l1', 'l2', 'elasticnet', and 'None'.\n",
        "        # Since our preprocessed data doesn't require elasticnet or no penalty,\n",
        "        # and we are primarily interested in comparing solvers with default L2 penalty,\n",
        "        # we will use the default penalty ('l2') where supported, and handle liblinear specifically.\n",
        "\n",
        "        if solver == 'liblinear':\n",
        "            # liblinear is good for small datasets and supports L1/L2\n",
        "            model = LogisticRegression(random_state=42, solver=solver)\n",
        "        elif solver == 'lbfgs':\n",
        "             # lbfgs is the default and generally recommended for medium to large datasets with L2\n",
        "             model = LogisticRegression(random_state=42, solver=solver, max_iter=1000) # Increased max_iter for convergence\n",
        "        elif solver == 'saga':\n",
        "            # saga is suitable for large datasets and supports L1/L2/ElasticNet/None\n",
        "            model = LogisticRegression(random_state=42, solver=solver, max_iter=1000) # Increased max_iter for convergence\n",
        "        else:\n",
        "            # Fallback for other potential solvers not explicitly handled\n",
        "            model = LogisticRegression(random_state=42, solver=solver)\n",
        "\n",
        "\n",
        "        # Fit the model to the training data\n",
        "        model.fit(X_train, y_train)\n",
        "        print(f\"Model trained successfully with {solver}.\")\n",
        "\n",
        "        # Predict on the test set\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        accuracy_by_solver[solver] = accuracy\n",
        "        print(f\"Accuracy with {solver} solver: {accuracy}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while training with solver {solver}: {e}\")\n",
        "        accuracy_by_solver[solver] = \"Error\"\n",
        "\n",
        "\n",
        "print(\"\\n--- Comparison of Solvers ---\")\n",
        "for solver, accuracy in accuracy_by_solver.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression models with different solvers...\n",
            "\n",
            "Training with solver: liblinear\n",
            "Model trained successfully with liblinear.\n",
            "Accuracy with liblinear solver: 0.8044692737430168\n",
            "\n",
            "Training with solver: lbfgs\n",
            "Model trained successfully with lbfgs.\n",
            "Accuracy with lbfgs solver: 0.8044692737430168\n",
            "\n",
            "Training with solver: saga\n",
            "Model trained successfully with saga.\n",
            "Accuracy with saga solver: 0.6703910614525139\n",
            "\n",
            "--- Comparison of Solvers ---\n",
            "Solver: liblinear, Accuracy: 0.8044692737430168\n",
            "Solver: lbfgs, Accuracy: 0.8044692737430168\n",
            "Solver: saga, Accuracy: 0.6703910614525139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "TRxG2xTbKCOg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76f4ff8c",
        "outputId": "0506ba2a-60e2-4daf-a0ec-35b6d7ff6c60"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset after handling missing values and encoding.\n",
        "\n",
        "# If you are running this cell independently, make sure X_train, X_test, y_train, y_test are defined.\n",
        "# You can uncomment and run the data loading and preprocessing steps from Q15 here if needed:\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "#\n",
        "# # Define the URL of the Titanic dataset\n",
        "# titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "# # Load the dataset\n",
        "# titanic_df = pd.read_csv(titanic_url)\n",
        "# # Handle missing values (using median for Age, mode for Embarked, drop Cabin)\n",
        "# titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)\n",
        "# titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0], inplace=True)\n",
        "# titanic_df.drop(columns=['Cabin'], inplace=True)\n",
        "# # Select features and target\n",
        "# features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "# target = 'Survived'\n",
        "# X = titanic_df[features]\n",
        "# y = titanic_df[target]\n",
        "# # Handle Categorical Features\n",
        "# X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "# # Split the dataset\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# # Optional: Apply StandardScaler if you want to evaluate MCC on scaled data as well\n",
        "# # scaler = StandardScaler()\n",
        "# # X_train_scaled = scaler.fit_transform(X_train)\n",
        "# # X_test_scaled = scaler.transform(X_test)\n",
        "# # Use X_train_scaled, X_test_scaled for training/prediction if scaling is desired.\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model_mcc = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear for simplicity\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_mcc.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained for Matthews Correlation Coefficient evaluation.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_mcc = model_mcc.predict(X_test)\n",
        "\n",
        "# Calculate the Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred_mcc)\n",
        "\n",
        "print(f\"\\nMatthews Correlation Coefficient (MCC): {mcc}\")\n",
        "\n",
        "# Interpretation of MCC:\n",
        "# MCC is a single value ranging from -1 to +1.\n",
        "# +1 represents a perfect prediction.\n",
        "# 0 represents no better than random prediction.\n",
        "# -1 represents a perfect inverse prediction.\n",
        "# MCC is generally considered a balanced measure that can be used even if the classes are of very different sizes."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained for Matthews Correlation Coefficient evaluation.\n",
            "\n",
            "Matthews Correlation Coefficient (MCC): 0.5798532412842439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "La1dj1npKTKI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a62dd77",
        "outputId": "cb448636-20a8-4557-8809-1ad881f2fb1a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X and y are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset\n",
        "# after handling missing values and encoding categorical variables.\n",
        "\n",
        "# Split the preprocessed dataset into training and testing sets\n",
        "# We'll use the same split for both raw and scaled data to ensure a fair comparison.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Dataset split into training and testing sets.\")\n",
        "print(f\"Training set shape (X_train): {X_train.shape}\")\n",
        "print(f\"Testing set shape (X_test): {X_test.shape}\")\n",
        "\n",
        "# --- Train Logistic Regression on RAW Data ---\n",
        "print(\"\\nTraining Logistic Regression model on RAW data...\")\n",
        "model_raw = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear for simplicity\n",
        "model_raw.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained on RAW data.\")\n",
        "\n",
        "# Predict and evaluate on raw test data\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "print(f\"Model Accuracy on RAW data: {accuracy_raw}\")\n",
        "\n",
        "# --- Apply Feature Scaling (Standardization) ---\n",
        "print(\"\\nApplying Feature Scaling (Standardization)...\")\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling applied.\")\n",
        "print(f\"Scaled Training set shape (X_train_scaled): {X_train_scaled.shape}\")\n",
        "print(f\"Scaled Testing set shape (X_test_scaled): {X_test_scaled.shape}\")\n",
        "\n",
        "# --- Train Logistic Regression on SCALED Data ---\n",
        "print(\"\\nTraining Logistic Regression model on SCALED data...\")\n",
        "# Using a solver that performs well with scaled data, like 'lbfgs' or 'saga'\n",
        "# Increased max_iter for convergence with scaled data, although liblinear is less sensitive\n",
        "model_scaled = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Model trained on SCALED data.\")\n",
        "\n",
        "# Predict and evaluate on scaled test data\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Model Accuracy on SCALED data: {accuracy_scaled}\")\n",
        "\n",
        "# --- Compare Results ---\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Accuracy on RAW data: {accuracy_raw}\")\n",
        "print(f\"Accuracy on SCALED data: {accuracy_scaled}\")\n",
        "\n",
        "if accuracy_scaled > accuracy_raw:\n",
        "    print(\"Applying scaling improved the model accuracy.\")\n",
        "elif accuracy_scaled < accuracy_raw:\n",
        "    print(\"Applying scaling decreased the model accuracy.\")\n",
        "else:\n",
        "    print(\"Applying scaling did not change the model accuracy significantly.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into training and testing sets.\n",
            "Training set shape (X_train): (712, 8)\n",
            "Testing set shape (X_test): (179, 8)\n",
            "\n",
            "Training Logistic Regression model on RAW data...\n",
            "Model trained on RAW data.\n",
            "Model Accuracy on RAW data: 0.8044692737430168\n",
            "\n",
            "Applying Feature Scaling (Standardization)...\n",
            "Feature scaling applied.\n",
            "Scaled Training set shape (X_train_scaled): (712, 8)\n",
            "Scaled Testing set shape (X_test_scaled): (179, 8)\n",
            "\n",
            "Training Logistic Regression model on SCALED data...\n",
            "Model trained on SCALED data.\n",
            "Model Accuracy on SCALED data: 0.8044692737430168\n",
            "\n",
            "--- Comparison ---\n",
            "Accuracy on RAW data: 0.8044692737430168\n",
            "Accuracy on SCALED data: 0.8044692737430168\n",
            "Applying scaling did not change the model accuracy significantly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "D-i4BfUMKpHi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aee90d5f",
        "outputId": "37c35aa5-156d-4303-a1b2-c2bdde816a08"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X and y are already defined and preprocessed from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset\n",
        "# after handling missing values and encoding categorical variables.\n",
        "# If you are running this cell independently, make sure X and y are defined\n",
        "# by running the necessary data loading and preprocessing steps.\n",
        "\n",
        "\n",
        "# Define the parameter grid for GridSearchCV, focusing on the 'C' parameter\n",
        "param_grid_c = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]  # Range of regularization strengths to test\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "# Using 'liblinear' solver as it is efficient for smaller datasets and supports L2 penalty (default).\n",
        "# For larger datasets or different penalties, other solvers might be more appropriate.\n",
        "model_c_tune = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# Set up Stratified K-Fold Cross-Validation\n",
        "# Stratification is important for classification tasks to ensure folds have similar class distributions.\n",
        "# We'll use 5 splits as a common practice.\n",
        "skf_c = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Create GridSearchCV object\n",
        "# We use the stratified k-fold cross-validation object (cv=skf_c).\n",
        "# The scoring metric is 'accuracy'.\n",
        "grid_search_c = GridSearchCV(model_c_tune, param_grid_c, cv=skf_c, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the entire dataset (X, y) to find the best C using cross-validation\n",
        "# Alternatively, you could fit on the training set if you've already split your data and want to keep a separate test set for final evaluation.\n",
        "print(\"Starting GridSearchCV to find optimal C...\")\n",
        "grid_search_c.fit(X, y)\n",
        "print(\"GridSearchCV finished.\")\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params_c = grid_search_c.best_params_\n",
        "best_accuracy_c = grid_search_c.best_score_ # This is the mean cross-validation accuracy for the best C\n",
        "\n",
        "# Print the best parameter (C) and the corresponding cross-validation accuracy\n",
        "print(f\"\\nOptimal C found by GridSearchCV: {best_params_c['C']}\")\n",
        "print(f\"Best Cross-validation Accuracy for optimal C: {best_accuracy_c}\")\n",
        "\n",
        "# Optionally, train the model with the optimal C on the full dataset or evaluate on a separate test set\n",
        "# For demonstration, let's train a final model with the optimal C on the full dataset\n",
        "# final_model = LogisticRegression(C=best_params_c['C'], random_state=42, solver='liblinear')\n",
        "# final_model.fit(X, y)\n",
        "# print(\"\\nFinal model trained on the full dataset with optimal C.\")\n",
        "\n",
        "# If you had a separate test set (X_test, y_test) from previous steps, you could evaluate the best_model on it:\n",
        "# best_model_c = grid_search_c.best_estimator_\n",
        "# y_pred_optimal_c = best_model_c.predict(X_test)\n",
        "# test_accuracy_optimal_c = accuracy_score(y_test, y_pred_optimal_c)\n",
        "# print(f\"\\nTest Set Accuracy with Optimal C: {test_accuracy_optimal_c}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GridSearchCV to find optimal C...\n",
            "GridSearchCV finished.\n",
            "\n",
            "Optimal C found by GridSearchCV: 1\n",
            "Best Cross-validation Accuracy for optimal C: 0.7957441466323522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "iPITWF3hK2ZW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70dee894",
        "outputId": "5693e03d-07a3-4a0f-c9ce-08d233859ff5"
      },
      "source": [
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "# Assuming X_train, X_test, y_train, and y_test are already defined and preprocessed\n",
        "# from previous cells (e.g., from Q15).\n",
        "# These variables should contain the features and target from the Titanic dataset\n",
        "# after handling missing values and encoding categorical variables.\n",
        "\n",
        "# If you are running this cell independently, make sure X_train, X_test, y_train, y_test are defined\n",
        "# by running the necessary data loading and preprocessing steps.\n",
        "# For example, uncomment the following block if you need to load and preprocess data:\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "#\n",
        "# # Define the URL of the Titanic dataset\n",
        "# titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "# # Load the dataset\n",
        "# titanic_df = pd.read_csv(titanic_url)\n",
        "# # Handle missing values (using median for Age, mode for Embarked, drop Cabin)\n",
        "# titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)\n",
        "# titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0], inplace=True)\n",
        "# titanic_df.drop(columns=['Cabin'], inplace=True)\n",
        "# # Select features and target\n",
        "# features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "# target = 'Survived'\n",
        "# X = titanic_df[features]\n",
        "# y = titanic_df[target]\n",
        "# # Handle Categorical Features\n",
        "# X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "# # Split the dataset\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# # Optional: Apply StandardScaler if you want to train/save/load a scaled model\n",
        "# # scaler = StandardScaler()\n",
        "# # X_train = scaler.fit_transform(X_train)\n",
        "# # X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Define the filename for saving the model\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "\n",
        "# --- Train Logistic Regression model ---\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_to_save = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear for simplicity\n",
        "model_to_save.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Save the trained model using joblib ---\n",
        "print(f\"\\nSaving the trained model to '{model_filename}'...\")\n",
        "joblib.dump(model_to_save, model_filename)\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# --- Load the saved model ---\n",
        "print(f\"\\nLoading the model from '{model_filename}'...\")\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# --- Make predictions using the loaded model ---\n",
        "print(\"\\nMaking predictions using the loaded model...\")\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Evaluate the loaded model's performance (optional) ---\n",
        "# You can compare the accuracy of the loaded model with the original model's performance\n",
        "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "print(f\"\\nAccuracy of the loaded model: {accuracy_loaded}\")\n",
        "\n",
        "# You can also compare predictions from the original model and the loaded model\n",
        "# y_pred_original = model_to_save.predict(X_test)\n",
        "# print(\"\\nAre predictions from original and loaded models the same?\", (y_pred_original == y_pred_loaded).all())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Saving the trained model to 'logistic_regression_model.joblib'...\n",
            "Model saved successfully.\n",
            "\n",
            "Loading the model from 'logistic_regression_model.joblib'...\n",
            "Model loaded successfully.\n",
            "\n",
            "Making predictions using the loaded model...\n",
            "Predictions made.\n",
            "\n",
            "Accuracy of the loaded model: 0.8044692737430168\n"
          ]
        }
      ]
    }
  ]
}